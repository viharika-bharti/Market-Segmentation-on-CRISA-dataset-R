---
title: "clustering, Assgt 3 getting started"
author: "Viharika"
date: "3/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
library(tidyverse)
library(factoextra)
library(readxl)
bsData <- read_excel("C:/Users/vihar/Documents/572/Assignment3/Assgt3_BathSoap_Data.xls", sheet = "DM_Sheet")


#the data read in may contain empty rows, columns, so remove these
bsData<-bsData[1:600, 1:46]

#better to change the colNames which contain punctuation, space to _ (like affluence index,etc)
names(bsData) <- gsub("[[:punct:]]|\\s", "_", names(bsData))

#The data with '%' in values are read in as 'chr' type - change these to numeric
bsData[20:46]<-lapply(bsData[20:46],function(x)  as.numeric(sub("%", "e-2", x)))

#rename the data
bsd<- bsData

#for brLoyalty, calculate maxBr as max of purchase by different major brand (excl others)
bsd<-bsd %>% rowwise() %>%  mutate(maxBr=max(Br__Cd__57__144, Br__Cd__55, Br__Cd__272, Br__Cd__286, Br__Cd__24, Br__Cd__481, Br__Cd__352, Br__Cd__5))

#for purchase of promotion
bsd$pur_promotion <- (1 - bsd$Pur_Vol_No_Promo____)


#plot(bsd$PropCat_10)
#length(which(bsd$PropCat_10 != 0))

#hist(bsd$PropCat_15)



```


Data exploration, cleaning
```{r}

#Examine the data - can all attributes be considered as 'numeric'
summary(as.factor(bsd$FEH))

#convert this to dummies, since the values are not ordinal, and remove the '0' level dummy
bsd<-bsd %>% mutate(fehDummy=1) %>% pivot_wider(names_from = FEH, values_from = fehDummy, names_prefix = "FEH_", values_fill = list(fehDummy=0))
bsd<- bsd %>% select(-FEH_0)  # can append this to the last line too

#explore MT
summary(as.factor(bsd$MT))
#keep levels 0, 4, 5, 10, 25 as dummies, with 0 in the dummies indicating 'other'
bsd<- bsd %>% mutate(MT=if_else(MT %in% c(0, 4, 5, 10, 17), MT, -1))
bsd<-bsd %>% mutate(mtDummy=1) %>% pivot_wider(names_from = MT, values_from = mtDummy, names_prefix = "MT_", values_fill = list(mtDummy=0)) 
bsd<- bsd %>% select(- `MT_-1`)

#similarly for CHILD, leave out the level '5' for unknown
bsd<-bsd %>% mutate(mtChild=1) %>% pivot_wider(names_from = CHILD, values_from = mtChild, names_prefix = "CHILD_", values_fill = list(mtChild=0)) %>% select(- CHILD_5) 

#SEX, dummies

bsd<-bsd %>% mutate(SEXDummy=1) %>% pivot_wider(names_from = SEX, values_from = SEXDummy, names_prefix = "SEX_", values_fill = list(SEXDummy=0)) %>% select(- SEX_1) 

#AGE
bsd<-bsd %>% mutate(AGEDummy=1) %>% pivot_wider(names_from = AGE, values_from = AGEDummy, names_prefix = "AGE_", values_fill = list(AGEDummy=0)) %>% select(- AGE_1)

#EDU
summary(as.factor(bsd$EDU))

bsd<- bsd %>% mutate(EDU=if_else(EDU %in% c(0, 1, 4,5, 7), EDU, -1))
bsd<-bsd %>% mutate(EDUDummy=1) %>% pivot_wider(names_from = EDU, values_from = EDUDummy, names_prefix = "EDU_", values_fill = list(EDUDummy=0)) 
bsd<- bsd %>% select(- `EDU_-1`)

#HS
summary(as.factor(bsd$HS))
bsd<- bsd %>% mutate(HS=if_else(HS %in% c(0,3,4, 5,6), HS, -1))
bsd<-bsd %>% mutate(HSDummy=1) %>% pivot_wider(names_from = HS, values_from = HSDummy, names_prefix = "HS_", values_fill = list(HSDummy=0)) 
bsd<- bsd %>% select(- `HS_-1`)


# CS, dummies
bsd<-bsd %>% mutate(CSDummy=1) %>% pivot_wider(names_from = CS, values_from = CSDummy, names_prefix = "CS_", values_fill = list(CSDummy=0))%>% select(- CS_2) 



```

kMeans clustering for purchase behaviour
```{r}
library(factoextra)
   #https://www.rdocumentation.org/packages/factoextra/versions/1.0.3

#kmeans -- https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/kmeans

######2.a)

#clustering on  purchase behavior varables
PURCHASE_BEHAVIOR <- c('No__of_Brands', 'Brand_Runs', 'Total_Volume', 'No__of__Trans', 'Value', 'Avg__Price', 'maxBr', 'Others_999')

x<- bsd
kmClus_pb<- x %>% select(PURCHASE_BEHAVIOR) %>% scale() %>%kmeans(centers=3, nstart=30)
#nstart = if centers is a number, how many random sets should be chosen? Can also change the iterations
#Or create a scaled dataset for clustering, and use this
xpb<-x %>% select(PURCHASE_BEHAVIOR) %>% scale()
kmClus_pb

#visualize the cluster - based on variables used for clustering
fviz_cluster(kmClus_pb, data=x %>% select(PURCHASE_BEHAVIOR))

  #https://www.rdocumentation.org/packages/factoextra/versions/1.0.6/topics/fviz_cluster


#add the cluster variable to the data and check the cluster descriptions in terms of broader set of variables
x <- x %>% mutate(clusKM=kmClus_pb$cluster)

x %>% group_by(clusKM) %>% summarise_at(c('FEH_1','FEH_2','FEH_3','HS_0','HS_3','HS_4','HS_5','HS_6','AGE_2','AGE_3','AGE_4','EDU_0','EDU_1','EDU_4','EDU_5','EDU_7', 'SEX_0','SEX_2','CS_0','CS_1','Affluence_Index','CHILD_1', 'CHILD_2', 'CHILD_3', 'CHILD_4', 'maxBr', 'No__of_Brands', 'No__of__Trans', 'Brand_Runs', 'Total_Volume', 'Value', 'Trans___Brand_Runs'), mean) %>% view()

PURCHASE_BEHAVIOR <- c('No__of_Brands', 'Brand_Runs', 'Total_Volume', 'No__of__Trans', 'Value', 'Trans___Brand_Runs', 'Vol_Tran', 'Avg__Price', 'maxBr', 'Others_999')

x<- bsd

kmClus_pb<- x %>% select(PURCHASE_BEHAVIOR) %>% scale() %>%kmeans(centers=3,nstart=30)

xpb<-x %>% select(PURCHASE_BEHAVIOR) %>% scale() 

x <- x %>% mutate(clusKM=kmClus_pb$cluster)


interpret <- as.data.frame(x %>% group_by(clusKM) %>% summarise_at(c('No__of_Brands', 'Brand_Runs', 'Total_Volume', 'No__of__Trans', 'Value', 'Trans___Brand_Runs', 'Vol_Tran', 'Avg__Price', 'maxBr', 'Others_999', 'HS_4','HS_6','HS_0','HS_5','HS_3','HS_4','CS_1', 'Affluence_Index'), mean ) %>% view())


library(GGally)
library(plotly)

p <- ggparcoord(data = interpret, columns = (2:23), mapping=aes(color=as.factor(clusKM)), groupColumn = "clusKM", scale = "std") + labs(x = "variable", y = "value", title = "Clustering") + scale_color_discrete("Clusters",labels=levels(interpret$clusKM)) + theme(axis.text.x = element_text(angle = 90))

ggplotly(p)

set.seed(9)

#converting to factor
x$clusKM <- as.factor(x$clusKM)

#no of rows
nr<-nrow(x)

#Splitting into training and testing
trnIndex = sample(1:nr, size = round(0.7*nr), replace=FALSE) 

#training and testing
xTrn <- x[trnIndex, ] 
xTst <- x[-trnIndex, ]

library(rpart)
DTree<- rpart(clusKM ~., data=xTrn, method="class", parms = list(split = "information"), control = rpart.control(minsplit = 30, cp=0.0001))

#Veiw the tree -> This is a basic tree, if you need fancy stuff I can work on it.
rpart.plot::prp(DTree, type=2, extra=1)

#Confusion table for training data
predTrn=predict(DTree, xTrn, type='class')
library(caret)
CM<- confusionMatrix(predTrn, xTrn$clusKM)
CM$table

#Accuracy on training data
Accuracy <- c(round(mean(predTrn==xTrn$clusKM),2))
Accuracy

#Pruning the tree - NOTE THIS IS OPTIONAL BEACUSE THE TREE IS SOO SMALL
#DTreePr<- prune.rpart(DTree, cp=0.0003)

#Confusion matrix for testing data
predTst=predict(DTree, xTst, type='class')
CMTest <- confusionMatrix(predTst, xTst$clusKM)
CMTest$table

#test Accuracy
Test_Accuracy<- round(mean(predTst==xTst$clusKM),2)
Test_Accuracy




#how many clusters is best
fviz_nbclust(xpb, kmeans, method = "wss")
fviz_nbclust(xpb, kmeans, method = "silhouette") 

##PAM - Partitioning around mediods (to avoid outliers)
library(cluster)

  #pam -- https://www.rdocumentation.org/packages/cluster/versions/2.1.0/topics/pam
# using euclidean distancing
pam_pb<-pam(xpb, k=4, metric = "euclidean")
#Partitioning Around Mediods

#pam_pb
#pam_pb$clusinfo

fviz_cluster(pam_pb)

# using manhattan distancing
pam_pb<-pam(xpb, k=5, metric = "manhattan")
fviz_cluster(pam_pb)



#silhoutte plot - using the silhoutte function in the cluster package
   #https://www.rdocumentation.org/packages/cluster/versions/2.1.0/topics/silhouette
si <- silhouette(pam_pb)
summary(si)
plot(si, col=1:3, border=NA)

```

For basis of purchase
```{r}
#### 2.b)
library(factoextra)

#clustering on  basis of purchase varables
#BASIS_OF_PURCHASE <- c('Pr_Cat_1', 'Pr_Cat_2', 'Pr_Cat_3' ,'Pr_Cat_4', 'Pur_Vol_No_Promo____', 'Pur_Vol_Promo_6__', 'Pur_Vol_Other_Promo__', 'PropCat_5', 'PropCat_6', 'PropCat_7', 'PropCat_8')

BASIS_OF_PURCHASE <- c('Pr_Cat_1', 'Pr_Cat_2', 'Pr_Cat_3' ,'Pr_Cat_4', 'pur_promotion', 'PropCat_5', 'PropCat_6', 'PropCat_7', 'PropCat_8')

y<- bsd
kmClus_bp<- y %>% select(BASIS_OF_PURCHASE) %>% scale() %>%kmeans(centers=3, nstart=25)
#nstart = if centers is a number, how many random sets should be chosen? Can also change the iterations
#Or create a scaled dataset for clustering, and use this
ybp<-y %>% select(BASIS_OF_PURCHASE) %>% scale()
kmClus_bp

#visualize the cluster - based on variables used for clustering
fviz_cluster(kmClus_bp, data=y %>% select(BASIS_OF_PURCHASE))




#add the cluster variable to the data and check the cluster descriptions in terms of broader set of variables
y <- y %>% mutate(clusKM=kmClus_bp$cluster)

#y %>% group_by(clusKM) %>% summarise_at(c('HS_0','HS_3','HS_4','HS_5','HS_6','AGE_2','AGE_3','AGE_4','EDU_0','EDU_1','EDU_4','EDU_5','EDU_7', 'SEX_0','SEX_2','CS_0','CS_1','Affluence_Index','CHILD_1', 'CHILD_2', 'CHILD_3', 'CHILD_4', 'maxBr', 'No__of_Brands', 'No__of__Trans', 'Brand_Runs', 'Total_Volume', 'Value', 'Trans___Brand_Runs'), mean) %>% view()

#how many clusters is best
fviz_nbclust(ybp, kmeans, method = "wss")
fviz_nbclust(ybp, kmeans, method = "silhouette") 



```
For purchase behavior and basis of purchase

```{r}
#### 2.c)

#clustering on  basis of purchase varables
All_VARIABLES <- c('No__of_Brands', 'Brand_Runs', 'Total_Volume', 'No__of__Trans', 'Value', 'Avg__Price', 'maxBr', 'Others_999','Pr_Cat_1', 'Pr_Cat_2', 'Pr_Cat_3' ,'Pr_Cat_4', 'Pur_Vol_No_Promo____', 'Pur_Vol_Promo_6__', 'Pur_Vol_Other_Promo__', 'PropCat_5', 'PropCat_6', 'PropCat_7', 'PropCat_8')

z<- bsd
kmClus_bp<- z %>% select(All_VARIABLES) %>% scale() %>%kmeans(centers=3, nstart=25)
#nstart = if centers is a number, how many random sets should be chosen? Can also change the iterations
#Or create a scaled dataset for clustering, and use this
pbbp<-z %>% select(All_VARIABLES) %>% scale()
kmClus_bp

#visualize the cluster - based on variables used for clustering
fviz_cluster(kmClus_bp, data=z %>% select(All_VARIABLES))




#add the cluster variable to the data and check the cluster descriptions in terms of broader set of variables
z <- z %>% mutate(clusKM=kmClus_bp$cluster)

#y %>% group_by(clusKM) %>% summarise_at(c('HS_0','HS_3','HS_4','HS_5','HS_6','AGE_2','AGE_3','AGE_4','EDU_0','EDU_1','EDU_4','EDU_5','EDU_7', 'SEX_0','SEX_2','CS_0','CS_1','Affluence_Index','CHILD_1', 'CHILD_2', 'CHILD_3', 'CHILD_4', 'maxBr', 'No__of_Brands', 'No__of__Trans', 'Brand_Runs', 'Total_Volume', 'Value', 'Trans___Brand_Runs'), mean) %>% view()

#how many clusters is best
fviz_nbclust(pbbp, kmeans, method = "wss")
fviz_nbclust(pbbp, kmeans, method = "silhouette") 

```
Hierarchical clustering

## Different distancing method we could try between records:- canberra,euclidean,manhattan,maximum

## Measuring distance between clusters:- average, complete, ward.D
```{r}


#1. euclidean
library(clusterSim)
xdist_euc <- dist(pbbp, method = "euclidean")
hierC_pb <- hclust(xdist_euc, method = "average" )
plot(hierC_pb, cex=0.3, hang=-3, main="hclust-average")

hierC_pb_c <- hclust(xdist_euc, method = "complete" )
plot(hierC_pb_c, cex=0.3, hang=-3, main="hclust-complete")

hierC_pb_w <- hclust(xdist_euc, method = "ward.D" )
plot(hierC_pb_w, cex=0.3, hang=-3, main="hclust-ward.D")

#using agnes from the cluster package
hierC_pb_ag_c <- agnes(xdist_euc, method = "complete")
plot(hierC_pb_ag_c)

hierC_pb_ag_w <- agnes(xdist_euc, method = "ward")
plot(hierC_pb_ag_w)

# Davies-Bouldin’s index

cl2 <- pam(pbbp, 3)
print(index.DB(pbbp, cl2$clustering, centrotypes="centroids"))

#check the agglomerative coeff given by agnes
hierC_pb_ag_c$ac
hierC_pb_ag_w$ac

#2. manhattan

xdist_manh <- dist(pbbp, method = "manhattan")
hierC_pb <- hclust(xdist_manh, method = "average" )
plot(hierC_pb, cex=0.3, hang=-3, main="hclust-average")

hierC_pb_c <- hclust(xdist_manh, method = "complete" )
plot(hierC_pb_c, cex=0.3, hang=-3, main="hclust-complete")

hierC_pb_w <- hclust(xdist_manh, method = "ward.D" )
plot(hierC_pb_w, cex=0.3, hang=-3, main="hclust-ward.D")

#using agnes from the cluster package
hierC_pb_ag_c <- agnes(xdist_manh, method = "complete" )
plot(hierC_pb_ag_c)

hierC_pb_ag_w <- agnes(xdist_manh, method = "ward" )
plot(hierC_pb_ag_w)

#check the agglomerative coeff given by agnes
hierC_pb_ag_c$ac
hierC_pb_ag_w$ac

#3. maximum

xdist_max <- dist(pbbp, method = "maximum")
hierC_pb <- hclust(xdist_max, method = "average" )
plot(hierC_pb, cex=0.3, hang=-3, main="hclust-average")

hierC_pb_c <- hclust(xdist_max, method = "complete" )
plot(hierC_pb_c, cex=0.3, hang=-3, main="hclust-complete")

hierC_pb_w <- hclust(xdist_max, method = "ward.D" )
plot(hierC_pb_w, cex=0.3, hang=-3, main="hclust-ward.D")

#using agnes from the cluster package
hierC_pb_ag_c <- agnes(xdist_manh, method = "complete" )
plot(hierC_pb_ag_c)

hierC_pb_ag_w <- agnes(xdist_manh, method = "ward" )
plot(hierC_pb_ag_w)



#check the agglomerative coeff given by agnes
hierC_pb_ag_c$ac
hierC_pb_ag_w$ac




#use cuttree to assign different clusters to examples
cut3_hierC_pb_ac_c <- cutree(hierC_pb_ag_c, k = 2)
table(cut3_hierC_pb_ac_c)
fviz_cluster(list(data=pbbp,cluster=cut3_hierC_pb_ac_c ), main="agnes-complete")

cut3_hierC_pb_ac_w <- cutree(hierC_pb_ag_w, k = 3)
table(cut3_hierC_pb_ac_w)

fviz_cluster(list(data=pbbp,cluster=cut3_hierC_pb_ac_w ), main="agnes-ward")


#dendograms using fviz_dend
fviz_dend(hierC_pb_ag_w)

fviz_dend(hierC_pb_ag_w, k=3, color_labels_by_k = TRUE, rect=TRUE, main="agnes - Wards")

#circular dendogram
fviz_dend(hierC_pb_w, k=2, color_labels_by_k = TRUE, type="circular", rect=TRUE, main="agnes - Wards")



```


DBSCAN clustering - example using the 'multishapes' dataset in the 'factoextra' package
```{r}

data("multishapes")

#Plot the points
multishapes %>% ggplot(aes(x=x,y=y, col=as.factor(shape)))+geom_point()
 
msKMeans <- kmeans(multishapes[,1:2], 5, nstart = 25)

fviz_cluster(msKMeans, data = multishapes[,1:2], main="kMeans on multishapes")


#Now use dbscan 
library(dbscan)

#dbscan - https://www.rdocumentation.org/packages/dbscan/versions/1.1-5/topics/dbscan

msDbscan <- dbscan(bsd[,1:2], eps = 0.15, minPts = 5)

fviz_cluster(msDbscan, data=bsd[,1:2], geom="point", ellipse  = FALSE, main="dbscan eps=0.5, minPts=5")

#optimal eps value
kNNdistplot(bsd[,1:2], k=4)
#includes data points within Ɛ-radius of a data point.
## eps is this distance that the algorithm uses to decide on whether to club the two points together. We will make use of the average distances of every point to its k nearest neighbors. These k distances are then plotted in ascending order. The point where you see an elbow like bend corresponds to the optimal *eps* value. At this point, a sharp change in the distance occurs, and thus this point serves as a threshold.
    #https://www.rdocumentation.org/packages/dbscan/versions/1.1-5/topics/kNNdist

```


#DBSCAN on dataset

```{r}
data("bsd")


dbscan::kNNdistplot(dbds, k =  4)
abline(h = 0.4, lty = 2)




```

Kernel k-means purchase behavior
```{r}
library(kernlab)

  #kkmeans - https://www.rdocumentation.org/packages/kernlab/versions/0.9-29/topics/kkmeans

kkc_pb<-kkmeans( xpb,centers=3)
     #uses default values - rbf kernel, and automatically sets the kernel
kkc_pb

#the cluster assignments for examples is in kkc_pb@.Data - use this for vizualizing using fviz_cluster

fviz_cluster(list(data=xpb, cluster=kkc_pb@.Data), geom="points", main="kkmeans")


#polynomial kernel with degree 2
kkc_pb_p2<-kkmeans( xpb,centers=3, kernel='polydot', kpar=list(degree=3))
kkc_pb_p2
fviz_cluster(list(data=xpb, cluster=kkc_pb_p2@.Data), geom="points", main="kkmeans")

#rbf kernel with specified sigma parameter
kkc_pb_rbf<-kkmeans( xpb,centers=3, kernel='rbfdot', kpar=list(sigma=0.2 ))
kkc_pb_rbf
fviz_cluster(list(data=xpb, cluster=kkc_pb_rbf@.Data), geom="points", main="kkmeans")

```

Kernel k-means basis of purchase

```{r}
library(kernlab)



kkc_pbbp<-kkmeans( pbbp,centers=2)
     #uses default values - rbf kernel, and automatically sets the kernel
kkc_pbbp

#the cluster assignments for examples is in kkc_pbbp@.Data - use this for vizualizing using fviz_cluster

fviz_cluster(list(data=pbbp, cluster=kkc_pbbp@.Data), geom="points", main="kkmeans")


#polynomial kernel with degree 2
kkc_pbbp_p2<-kkmeans( pbbp,centers=2, kernel='polydot', kpar=list(degree=3))
kkc_pbbp_p2
fviz_cluster(list(data=pbbp, cluster=kkc_pbbp_p2@.Data), geom="points", main="kkmeans")

#rbf kernel with specified sigma parameter
kkc_pbbp_rbf<-kkmeans( pbbp,centers=3, kernel='rbfdot', kpar=list(sigma=0.2 ))
kkc_pbbp_rbf
fviz_cluster(list(data=pbbp, cluster=kkc_pbbp_rbf@.Data), geom="points", main="kkmeans")



```


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
